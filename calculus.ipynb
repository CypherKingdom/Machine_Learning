{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2256dda4",
      "metadata": {
        "id": "2256dda4"
      },
      "source": [
        "# Calculus and Optimization for Machine Learning\n",
        "\n",
        "This notebook covers fundamental calculus concepts and optimization techniques essential for machine learning, including limits, continuity, derivatives, partial derivatives, gradients, and multivariate calculus with practical applications in ML."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0597e4fe",
      "metadata": {
        "id": "0597e4fe"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Calculus forms the mathematical foundation for optimization in machine learning. Understanding derivatives, gradients, and optimization methods is crucial for training ML models effectively. This notebook explores:\n",
        "\n",
        "- Limits and Continuity\n",
        "- Derivatives and their applications\n",
        "- Partial Derivatives and Gradients\n",
        "- Multivariate Calculus\n",
        "- Optimization techniques (Gradient Descent)\n",
        "- Real-world ML applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "831449bb",
      "metadata": {
        "id": "831449bb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import linprog"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbd6b04c",
      "metadata": {
        "id": "cbd6b04c"
      },
      "source": [
        "## Limits\n",
        "\n",
        "**Definition:**\n",
        "A limit describes the value a function approaches as the input approaches some point.\n",
        "\n",
        "**Formally:**\n",
        "$$\\lim_{x \\to a} f(x) = L$$\n",
        "\n",
        "This means that as $x$ gets arbitrarily close to $a$, $f(x)$ gets arbitrarily close to $L$.\n",
        "\n",
        "**Why in ML?**\n",
        "- Helps in understanding optimization behavior (e.g., convergence in gradient descent)\n",
        "- Essential for analyzing algorithm convergence rates\n",
        "- Used to understand the behavior of loss functions at critical points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "727067ed",
      "metadata": {
        "id": "727067ed"
      },
      "source": [
        "## Continuity\n",
        "\n",
        "**Definition:**\n",
        "A function $f(x)$ is continuous at a point $x = a$ if:\n",
        "\n",
        "$$\\lim_{x \\to a} f(x) = f(a)$$\n",
        "\n",
        "This means:\n",
        "1. $f(a)$ is defined\n",
        "2. $\\lim_{x \\to a} f(x)$ exists\n",
        "3. The limit equals the function value\n",
        "\n",
        "**Why in ML?**\n",
        "- Ensures smoothness in loss functions for stable optimization\n",
        "- Continuous functions are easier to optimize\n",
        "- Discontinuities can cause gradient descent to fail or converge slowly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d739ae",
      "metadata": {
        "id": "54d739ae"
      },
      "source": [
        "## Derivatives\n",
        "\n",
        "**Definition:**\n",
        "The derivative measures the rate of change of a function with respect to its input:\n",
        "\n",
        "$$f'(x) = \\frac{df}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
        "\n",
        "**Interpretation:**\n",
        "- The derivative represents the slope of the tangent line to the function at a point\n",
        "- It tells us how the function changes when we make a small change to the input\n",
        "\n",
        "**Why in ML?**\n",
        "- Used in gradient descent to minimize loss functions\n",
        "- Essential for backpropagation in neural networks\n",
        "- Helps identify optimal parameters for models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be3db3f",
      "metadata": {
        "id": "8be3db3f"
      },
      "source": [
        "## Gradient Descent - Core Optimization Algorithm\n",
        "\n",
        "**What is Gradient Descent?**\n",
        "\n",
        "Gradient descent is an optimization algorithm used in machine learning and other fields to find the minimum of a function by iteratively moving in the direction of the steepest decrease.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "**Problem:** Minimize a loss function $L(\\theta)$\n",
        "\n",
        "**Solution:** Update rule using derivatives:\n",
        "\n",
        "$$\\theta_{new} = \\theta_{old} - \\alpha \\frac{dL}{d\\theta}$$\n",
        "\n",
        "where:\n",
        "- $\\theta$ represents the parameters we want to optimize\n",
        "- $\\alpha$ is the learning rate (step size)\n",
        "- $\\frac{dL}{d\\theta}$ is the gradient (derivative) of the loss function\n",
        "\n",
        "**Example:** Linear regression uses derivatives to find optimal weights.\n",
        "\n",
        "**Key Concepts:**\n",
        "- Gradient descent is a family of algorithms used to search for parameters that minimize the loss function\n",
        "- It iteratively adjusts parameters in the direction opposite to the gradient\n",
        "- The learning rate controls how big each step is"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b967f8b",
      "metadata": {
        "id": "7b967f8b"
      },
      "source": [
        "### Mean Square Error (MSE) / L2 Loss\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "where:\n",
        "- $y_i$ is the actual target value\n",
        "- $\\hat{y}_i$ is the predicted value\n",
        "- $n$ is the number of samples\n",
        "\n",
        "**Properties:**\n",
        "- Squaring the difference between predictions and actual target values results in a higher penalty assigned to larger deviations from the target value\n",
        "- Taking the mean normalizes the total error against the number of samples in a dataset\n",
        "- Commonly used in regression tasks\n",
        "- Differentiable, making it suitable for gradient-based optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64653c91",
      "metadata": {
        "id": "64653c91"
      },
      "source": [
        "## Partial Derivatives\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "For a function of multiple variables $f(x, y, z, ...)$, a partial derivative measures the rate of change with respect to one variable while keeping others constant.\n",
        "\n",
        "**Notation:**\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial x}, \\quad \\frac{\\partial f}{\\partial y}, \\quad \\frac{\\partial f}{\\partial z}$$\n",
        "\n",
        "**Example:**\n",
        "\n",
        "For $f(x, y) = x^2 + 3xy + y^2$:\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial x} = 2x + 3y$$\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial y} = 3x + 2y$$\n",
        "\n",
        "**Why in ML?**\n",
        "- Training neural networks requires computing partial derivatives for each weight\n",
        "- Each parameter is updated independently based on its partial derivative\n",
        "- Essential for backpropagation algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "466106bb",
      "metadata": {
        "id": "466106bb"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "The gradient is a vector of all partial derivatives of a multivariable function:\n",
        "\n",
        "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\n",
        "\n",
        "**Properties:**\n",
        "- The gradient points in the direction of steepest ascent\n",
        "- The negative gradient points in the direction of steepest descent\n",
        "- The magnitude of the gradient indicates how steep the slope is\n",
        "\n",
        "**Why in ML?**\n",
        "- Gradient descent uses the negative gradient to minimize loss functions\n",
        "- Backpropagation in deep learning computes gradients efficiently\n",
        "- Essential for optimizing high-dimensional parameter spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d11ae4d",
      "metadata": {
        "id": "3d11ae4d"
      },
      "source": [
        "## Real Example in ML: Neural Networks\n",
        "\n",
        "**Problem:** Optimize weights $W$ in a neural network\n",
        "\n",
        "**Solution:** Compute gradients of the loss with respect to each weight\n",
        "\n",
        "**Backpropagation:**\n",
        "- Uses the chain rule to compute gradients efficiently\n",
        "- Propagates error backwards through the network\n",
        "- Updates each weight based on its contribution to the total error\n",
        "\n",
        "**Update Rule:**\n",
        "\n",
        "$$W_{new} = W_{old} - \\alpha \\frac{\\partial L}{\\partial W}$$\n",
        "\n",
        "where $L$ is the loss function and $\\alpha$ is the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2249ac6b",
      "metadata": {
        "id": "2249ac6b"
      },
      "source": [
        "## Multivariate Calculus\n",
        "\n",
        "**Multivariate Functions:**\n",
        "\n",
        "**Definition:**\n",
        "- Functions with multiple inputs, e.g., $f(x, y, z)$\n",
        "\n",
        "**Why in ML?**\n",
        "- Most ML models have high-dimensional parameter spaces\n",
        "- Image recognition models process pixel arrays (hundreds or thousands of dimensions)\n",
        "- Natural language models handle word embeddings in high-dimensional spaces\n",
        "- Requires understanding of partial derivatives and gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a05bf75a",
      "metadata": {
        "id": "a05bf75a"
      },
      "source": [
        "## Optimization Basics\n",
        "\n",
        "**Goal:** Find the minimum (or maximum) of a function\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "1. **Local Minimum:** A point where the function value is smaller than all nearby points\n",
        "2. **Global Minimum:** The smallest value of the function over its entire domain\n",
        "3. **Critical Points:** Points where the gradient is zero ($\\nabla f = 0$)\n",
        "4. **Saddle Points:** Critical points that are neither minima nor maxima\n",
        "\n",
        "**Optimization Methods:**\n",
        "- Gradient Descent: First-order method using gradients\n",
        "- Newton's Method: Second-order method using Hessian matrix\n",
        "- Constrained Optimization: Using Lagrange multipliers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "515da716",
      "metadata": {
        "id": "515da716"
      },
      "source": [
        "## Real Example in ML: Support Vector Machines (SVM)\n",
        "\n",
        "**Problem:** Maximize the margin between classes\n",
        "\n",
        "**Solution:** Use Lagrange multipliers (multivariate optimization)\n",
        "\n",
        "**Formulation:**\n",
        "- Find the hyperplane that maximally separates two classes\n",
        "- Subject to constraints that all points are correctly classified\n",
        "- Involves solving a constrained optimization problem\n",
        "\n",
        "**Mathematical Setup:**\n",
        "$$\\min_{w,b} \\frac{1}{2} \\|w\\|^2$$\n",
        "\n",
        "Subject to: $y_i(w^T x_i + b) \\geq 1$ for all $i$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6587d644",
      "metadata": {
        "id": "6587d644"
      },
      "source": [
        "## Summary of Key Concepts\n",
        "\n",
        "**Core Principles:**\n",
        "- **Limits & Derivatives:** Foundation for optimization\n",
        "- **Partial Derivatives & Gradients:** Key for training ML models\n",
        "- **Multivariate Calculus:** Essential for high-dimensional optimization\n",
        "\n",
        "**Applications in ML:**\n",
        "- Gradient descent for minimizing loss functions\n",
        "- Backpropagation for training neural networks\n",
        "- Constrained optimization for SVMs\n",
        "- Convergence analysis for optimization algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53bc767c",
      "metadata": {
        "id": "53bc767c"
      },
      "source": [
        "## Comprehensive Concept Reference Table\n",
        "\n",
        "| Concept | Definition | Use in ML & Optimization | Real-Life Example |\n",
        "|---------|-----------|-------------------------|-------------------|\n",
        "| **Limits** | The value a function approaches as input approaches a point | Analyzing model convergence (e.g., gradient descent) | Predicting stock prices as time intervals shrink (instantaneous rate of change) |\n",
        "| **Continuity** | A function is continuous if small input changes cause small output changes | Ensuring smooth loss functions for stable training | Temperature sensor readings over time (no sudden jumps) |\n",
        "| **Derivatives** | Measures the rate of change of a function w.r.t. its input | Gradient descent for minimizing loss (e.g., linear regression) | Optimizing delivery routes by adjusting speed to minimize fuel costs |\n",
        "| **Partial Derivatives** | Derivative of a multivariable function w.r.t. one variable (others fixed) | Training neural networks (updating individual weights) | Adjusting thermostat settings for energy efficiency (keeping other factors constant) |\n",
        "| **Gradient** | Vector of partial derivatives of a multivariable function | Direction of steepest ascent/descent in optimization (e.g., backpropagation in deep learning) | Robot path planning to avoid obstacles (following the steepest safety gradient) |\n",
        "| **Multivariate Calculus** | Calculus applied to functions with multiple variables | Handling high-dimensional data (e.g., image recognition with pixel arrays) | Weather forecasting using multiple variables (temperature, humidity, pressure) |\n",
        "| **Optimization** | Finding minima/maxima of functions | Training all ML models (e.g., gradient descent, Adam optimizer) | warehouse inventory management (minimizing costs while meeting demand) |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e959639f",
      "metadata": {
        "id": "e959639f"
      },
      "source": [
        "# Practical Exercises and Solutions\n",
        "\n",
        "This section contains hands-on exercises to practice calculus and optimization concepts in machine learning contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e78b60",
      "metadata": {
        "id": "40e78b60"
      },
      "source": [
        "## Exercise 1: Gradient Descent for a Simple Quadratic Function\n",
        "\n",
        "**Problem:** Minimize the function $f(x) = x^2 + 3x + 2$ using gradient descent.\n",
        "\n",
        "**Steps:**\n",
        "1. Define the function $f(x) = x^2 + 3x + 2$\n",
        "2. Compute the gradient (derivative): $f'(x) = 2x + 3$\n",
        "3. Initialize $x$ with a starting value\n",
        "4. Iteratively update: $x_{new} = x_{old} - \\alpha \\cdot f'(x_{old})$\n",
        "5. Repeat until convergence\n",
        "\n",
        "**Expected Output:** The value of $x$ converges to $-1.5$ (the minimum of the quadratic function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9345c0ef",
      "metadata": {
        "id": "9345c0ef"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to minimize: f(x) = x² + 3x + 2\n",
        "def f(x):\n",
        "    return x**2 + 3*x + 2\n",
        "\n",
        "# Gradient of f(x): df/dx = 2x + 3\n",
        "def gradient(x):\n",
        "    return 2*x + 3\n",
        "\n",
        "# Gradient Descent\n",
        "x = 0  # Initial guess\n",
        "learning_rate = 0.1\n",
        "steps = 20\n",
        "\n",
        "print(\"Gradient Descent for f(x) = x² + 3x + 2\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for i in range(steps):\n",
        "    grad = gradient(x)\n",
        "    x = x - learning_rate * grad\n",
        "    print(f\"Step {i+1}: x = {x:.4f}, f(x) = {f(x):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"Optimal x: {x:.4f}\")\n",
        "print(f\"Minimum value: {f(x):.4f}\")\n",
        "print(f\"\\nAnalytical minimum: x = -1.5, f(-1.5) = {f(-1.5):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd5b5bba",
      "metadata": {
        "id": "bd5b5bba"
      },
      "source": [
        "## Exercise 2: Gradient Descent for Linear Regression (Single Variable)\n",
        "\n",
        "**Problem:** Fit a line $y = mx + b$ to some data using gradient descent.\n",
        "\n",
        "**Data:**\n",
        "```python\n",
        "X = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 4, 6, 8, 10])\n",
        "```\n",
        "\n",
        "**Steps:**\n",
        "1. Initialize parameters $m$ (slope) and $b$ (intercept) to 0\n",
        "2. Define the prediction: $\\hat{y} = mx + b$\n",
        "3. Use MSE loss: $L = \\frac{1}{n} \\sum (\\hat{y}_i - y_i)^2$\n",
        "4. Compute gradients:\n",
        "   - $\\frac{\\partial L}{\\partial m} = \\frac{2}{n} \\sum (\\hat{y}_i - y_i) \\cdot x_i$\n",
        "   - $\\frac{\\partial L}{\\partial b} = \\frac{2}{n} \\sum (\\hat{y}_i - y_i)$\n",
        "5. Update parameters using gradient descent\n",
        "\n",
        "**Expected Output:** The line should converge to $y = 2x + 0$ ($m \\approx 2.0$ and $b \\approx 0.0$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84ccb4c",
      "metadata": {
        "id": "d84ccb4c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "# Initialize parameters (slope m and intercept b)\n",
        "m, b = 0, 0\n",
        "learning_rate = 0.01\n",
        "steps = 100\n",
        "\n",
        "print(\"Linear Regression using Gradient Descent\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Data: X = {X}\")\n",
        "print(f\"      y = {y}\")\n",
        "print(\"\\nTraining Progress:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Gradient Descent\n",
        "for step in range(steps):\n",
        "    # Predictions: y_pred = m*X + b\n",
        "    y_pred = m * X + b\n",
        "\n",
        "    # Gradients (MSE loss)\n",
        "    grad_m = np.mean(2 * (y_pred - y) * X)  # dL/dm\n",
        "    grad_b = np.mean(2 * (y_pred - y))      # dL/db\n",
        "\n",
        "    # Update m and b\n",
        "    m -= learning_rate * grad_m\n",
        "    b -= learning_rate * grad_b\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        loss = np.mean((y_pred - y)**2)\n",
        "        print(f\"Step {step:3d}: m = {m:.4f}, b = {b:.4f}, Loss = {loss:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"\\nFinal line: y = {round(m, 2)} * x + {round(b, 2)}\")\n",
        "print(f\"\\nPredictions vs Actual:\")\n",
        "y_final = m * X + b\n",
        "for i in range(len(X)):\n",
        "    print(f\"x={X[i]}: predicted={y_final[i]:.2f}, actual={y[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce80d1a2",
      "metadata": {
        "id": "ce80d1a2"
      },
      "source": [
        "## Exercise 3: Visualizing Gradient Descent\n",
        "\n",
        "**Problem:** Plot how the loss decreases over time during gradient descent.\n",
        "\n",
        "**Function:** Use the simple function $y = x^2$\n",
        "\n",
        "**Visualization:**\n",
        "- Track the value of $x$ and $f(x)$ at each iteration\n",
        "- Plot the convergence of the loss over iterations\n",
        "- Show how the algorithm finds the minimum\n",
        "\n",
        "**Expected Output:** A plot showing the loss decreasing and converging to the minimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2890a288",
      "metadata": {
        "id": "2890a288"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function: f(x) = x²\n",
        "def f(x):\n",
        "    return x**2\n",
        "\n",
        "# Gradient: df/dx = 2x\n",
        "def gradient(x):\n",
        "    return 2*x\n",
        "\n",
        "# Gradient Descent\n",
        "x = 3  # Start at x=3\n",
        "learning_rate = 0.1\n",
        "steps = 20\n",
        "\n",
        "x_history = []\n",
        "loss_history = []\n",
        "\n",
        "for step in range(steps):\n",
        "    x_history.append(x)\n",
        "    loss_history.append(f(x))\n",
        "\n",
        "    grad = gradient(x)\n",
        "    x = x - learning_rate * grad\n",
        "\n",
        "# Plotting\n",
        "plt.plot(loss_history, 'bo-')\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss (f(x))\")\n",
        "plt.title(\"Gradient Descent Convergence\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Starting position: x = {x_history[0]:.4f}, f(x) = {loss_history[0]:.4f}\")\n",
        "print(f\"Final position: x = {x_history[-1]:.4f}, f(x) = {loss_history[-1]:.4f}\")\n",
        "print(f\"\\nConverged to minimum at x ≈ 0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "847fbf9c",
      "metadata": {
        "id": "847fbf9c"
      },
      "source": [
        "## Exercise 4: Linear Programming Optimization\n",
        "\n",
        "**Problem:** Solve a constrained optimization problem using linear programming.\n",
        "\n",
        "**Objective:** Maximize $f(x, y) = x + 2y$\n",
        "\n",
        "**Subject to constraints:**\n",
        "- $2x + y \\leq 20$\n",
        "- $-4x + 5y \\leq 10$\n",
        "- $x - 2y \\leq 2$\n",
        "- $x \\geq 0, y \\geq 0$\n",
        "\n",
        "**Method:** Use scipy's linear programming solver (linprog)\n",
        "\n",
        "**Note:** linprog minimizes by default, so we minimize $-x - 2y$ to maximize $x + 2y$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067b825c",
      "metadata": {
        "id": "067b825c"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import linprog\n",
        "import numpy as np\n",
        "\n",
        "# Objective function coefficients (minimize -x - 2y to maximize x + 2y)\n",
        "c = [-1, -2]\n",
        "\n",
        "# Inequality constraints: A_ub @ x <= b_ub\n",
        "A = [[2, 1],    # 2x + y <= 20\n",
        "     [-4, 5],   # -4x + 5y <= 10\n",
        "     [1, -2]]   # x - 2y <= 2\n",
        "\n",
        "b = [20, 10, 2]\n",
        "\n",
        "# Bounds for variables (x >= 0, y >= 0)\n",
        "x_bounds = (0, None)\n",
        "y_bounds = (0, None)\n",
        "\n",
        "print(\"Linear Programming Optimization\")\n",
        "print(\"=\"*60)\n",
        "print(\"Objective: Maximize f(x, y) = x + 2y\")\n",
        "print(\"\\nConstraints:\")\n",
        "print(\"  2x + y ≤ 20\")\n",
        "print(\"  -4x + 5y ≤ 10\")\n",
        "print(\"  x - 2y ≤ 2\")\n",
        "print(\"  x ≥ 0, y ≥ 0\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Solve the linear program\n",
        "result = linprog(c, A_ub=A, b_ub=b, bounds=[x_bounds, y_bounds], method='highs')\n",
        "\n",
        "if result.success:\n",
        "    print(\"\\nOptimization successful!\")\n",
        "    print(f\"\\nOptimal solution:\")\n",
        "    print(f\"  x = {result.x[0]:.4f}\")\n",
        "    print(f\"  y = {result.x[1]:.4f}\")\n",
        "    print(f\"\\nMaximum value: f(x, y) = {-result.fun:.4f}\")\n",
        "\n",
        "    # Verify constraints\n",
        "    print(\"\\nConstraint verification:\")\n",
        "    x_opt, y_opt = result.x\n",
        "    print(f\"  2x + y = {2*x_opt + y_opt:.4f} ≤ 20 ✓\")\n",
        "    print(f\"  -4x + 5y = {-4*x_opt + 5*y_opt:.4f} ≤ 10 ✓\")\n",
        "    print(f\"  x - 2y = {x_opt - 2*y_opt:.4f} ≤ 2 ✓\")\n",
        "else:\n",
        "    print(\"\\nOptimization failed!\")\n",
        "    print(f\"Reason: {result.message}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2915ae9",
      "metadata": {
        "id": "a2915ae9"
      },
      "source": [
        "## Additional Practice: Multivariate Gradient Descent\n",
        "\n",
        "**Problem:** Minimize a multivariate function using gradient descent.\n",
        "\n",
        "**Function:** $f(x, y) = x^2 + y^2 + 2x - 4y + 5$\n",
        "\n",
        "**Gradients:**\n",
        "- $\\frac{\\partial f}{\\partial x} = 2x + 2$\n",
        "- $\\frac{\\partial f}{\\partial y} = 2y - 4$\n",
        "\n",
        "**Expected minimum:** $x = -1, y = 2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f7e087f",
      "metadata": {
        "id": "0f7e087f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Function: f(x, y) = x² + y² + 2x - 4y + 5\n",
        "def f(x, y):\n",
        "    return x**2 + y**2 + 2*x - 4*y + 5\n",
        "\n",
        "# Gradient\n",
        "def gradient(x, y):\n",
        "    df_dx = 2*x + 2\n",
        "    df_dy = 2*y - 4\n",
        "    return np.array([df_dx, df_dy])\n",
        "\n",
        "# Gradient Descent\n",
        "position = np.array([5.0, 5.0])  # Starting point\n",
        "learning_rate = 0.1\n",
        "steps = 30\n",
        "\n",
        "history = [position.copy()]\n",
        "\n",
        "print(\"Multivariate Gradient Descent\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Function: f(x, y) = x² + y² + 2x - 4y + 5\")\n",
        "print(f\"Starting point: ({position[0]:.2f}, {position[1]:.2f})\")\n",
        "print(\"\\nOptimization progress:\")\n",
        "\n",
        "for i in range(steps):\n",
        "    grad = gradient(position[0], position[1])\n",
        "    position = position - learning_rate * grad\n",
        "    history.append(position.copy())\n",
        "\n",
        "    if i % 5 == 0:\n",
        "        print(f\"Step {i:2d}: x={position[0]:6.3f}, y={position[1]:6.3f}, f(x,y)={f(position[0], position[1]):6.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"\\nFinal position: x = {position[0]:.4f}, y = {position[1]:.4f}\")\n",
        "print(f\"Minimum value: f(x, y) = {f(position[0], position[1]):.4f}\")\n",
        "print(f\"\\nAnalytical minimum: x = -1, y = 2, f(-1, 2) = {f(-1, 2):.4f}\")\n",
        "\n",
        "# Visualization\n",
        "history = np.array(history)\n",
        "\n",
        "# Create meshgrid for contour plot\n",
        "x_range = np.linspace(-3, 6, 100)\n",
        "y_range = np.linspace(-2, 6, 100)\n",
        "X, Y = np.meshgrid(x_range, y_range)\n",
        "Z = f(X, Y)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Contour plot\n",
        "plt.subplot(1, 2, 1)\n",
        "contour = plt.contour(X, Y, Z, levels=20, cmap='viridis')\n",
        "plt.colorbar(contour)\n",
        "plt.plot(history[:, 0], history[:, 1], 'ro-', markersize=4, linewidth=1.5, label='Gradient Descent Path')\n",
        "plt.scatter([history[0, 0]], [history[0, 1]], color='green', s=100, zorder=5, label='Start')\n",
        "plt.scatter([history[-1, 0]], [history[-1, 1]], color='red', s=100, zorder=5, label='End')\n",
        "plt.scatter([-1], [2], color='yellow', s=150, marker='*', zorder=6, label='True Minimum')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Contour Plot with Gradient Descent Path')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 3D surface plot\n",
        "ax = plt.subplot(1, 2, 2, projection='3d')\n",
        "ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)\n",
        "ax.plot(history[:, 0], history[:, 1], [f(x, y) for x, y in history], 'ro-', markersize=4, linewidth=2)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('f(x, y)')\n",
        "ax.set_title('3D Surface with Optimization Path')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc1589b4",
      "metadata": {
        "id": "fc1589b4"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we covered:\n",
        "\n",
        "1. **Fundamental Concepts:**\n",
        "   - Limits and their role in understanding convergence\n",
        "   - Continuity for stable optimization\n",
        "   - Derivatives as the foundation of gradient-based methods\n",
        "\n",
        "2. **Gradient Descent:**\n",
        "   - Core optimization algorithm for machine learning\n",
        "   - Update rule using derivatives\n",
        "   - Applications to simple functions and linear regression\n",
        "\n",
        "3. **Multivariate Calculus:**\n",
        "   - Partial derivatives for functions of multiple variables\n",
        "   - Gradients as vectors pointing to steepest ascent\n",
        "   - Applications in neural networks and high-dimensional optimization\n",
        "\n",
        "4. **Practical Applications:**\n",
        "   - Gradient descent for quadratic functions\n",
        "   - Linear regression using gradient descent\n",
        "   - Visualization of convergence\n",
        "   - Constrained optimization using linear programming\n",
        "   - Multivariate optimization\n",
        "\n",
        "5. **Real-World ML Examples:**\n",
        "   - Neural network training via backpropagation\n",
        "   - Support Vector Machines with Lagrange multipliers\n",
        "   - Various optimization scenarios\n",
        "\n",
        "These concepts form the mathematical backbone of modern machine learning, enabling us to train models effectively and understand their behavior."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}