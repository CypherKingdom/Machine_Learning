{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5e2fa4ad",
      "metadata": {
        "id": "5e2fa4ad"
      },
      "source": [
        "# Principal Component Analysis (PCA)\n",
        "\n",
        "## Definition\n",
        "\n",
        "**Principal Component Analysis (PCA)** is a powerful dimensionality reduction technique widely used in data analysis, machine learning, and statistics. It transforms high-dimensional data into a lower-dimensional form while retaining most of the variance in the original dataset.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b388970a",
      "metadata": {
        "id": "b388970a"
      },
      "source": [
        "## Why PCA?\n",
        "\n",
        "### Key Motivations:\n",
        "\n",
        "1. **Curse of Dimensionality**: High-dimensional data is computationally expensive and difficult to visualize.\n",
        "2. **Noise Reduction**: PCA helps eliminate redundant features and noise.\n",
        "3. **Feature Extraction**: It identifies the most significant patterns in the data.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3049feb",
      "metadata": {
        "id": "a3049feb"
      },
      "source": [
        "## Applications of PCA\n",
        "\n",
        "- **Image Compression**: Reducing the dimensionality of image data while preserving visual quality\n",
        "- **Stock Market Trend Analysis**: Identifying key factors driving market movements\n",
        "- **Genetics**: Analyzing gene expression data and identifying patterns in genetic variation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "665c5311",
      "metadata": {
        "id": "665c5311"
      },
      "source": [
        "## Mathematical Foundations of PCA\n",
        "\n",
        "PCA relies on **linear algebra** concepts, particularly **eigenvalue decomposition** of the covariance matrix.\n",
        "\n",
        "### Steps in PCA:\n",
        "\n",
        "1. **Standardize data**\n",
        "2. **Compute covariance matrix**\n",
        "3. **Find eigenvalues & eigenvectors**\n",
        "4. **Select top-k principal components**\n",
        "5. **Project data onto these components**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8560f71c",
      "metadata": {
        "id": "8560f71c"
      },
      "source": [
        "## Detailed Steps with Formulas\n",
        "\n",
        "### Step 1: Standardize the Data\n",
        "\n",
        "**Purpose**: Ensure all features have the same scale to prevent features with larger magnitudes from dominating the analysis.\n",
        "\n",
        "**Formula**:\n",
        "$$X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}$$\n",
        "\n",
        "where:\n",
        "- $X$ is the original data\n",
        "- $\\mu$ is the mean of each feature\n",
        "- $\\sigma$ is the standard deviation of each feature\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Compute the Covariance Matrix\n",
        "\n",
        "**Purpose**: Measure how features vary together to understand relationships between variables.\n",
        "\n",
        "**Formula**:\n",
        "$$C = \\frac{1}{n-1} X^T X$$\n",
        "\n",
        "where:\n",
        "- $C$ is the covariance matrix (d × d for d features)\n",
        "- $X$ is the centered data matrix (n × d)\n",
        "- $n$ is the number of samples\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Find Eigenvalues & Eigenvectors\n",
        "\n",
        "**Purpose**: Identify the directions (eigenvectors) of maximum variance and their corresponding magnitudes (eigenvalues).\n",
        "\n",
        "**Formula**:\n",
        "$$C v = \\lambda v$$\n",
        "\n",
        "where:\n",
        "- $C$ is the covariance matrix\n",
        "- $v$ is an eigenvector (principal component direction)\n",
        "- $\\lambda$ is the corresponding eigenvalue (variance along that direction)\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: Select Top-k Principal Components\n",
        "\n",
        "**Purpose**: Choose the most important components based on explained variance.\n",
        "\n",
        "**Explained Variance Ratio**:\n",
        "$$\\text{Explained Variance} = \\frac{\\lambda_i}{\\sum_{j=1}^{d} \\lambda_j}$$\n",
        "\n",
        "Sort eigenvalues in descending order and select the top k eigenvectors.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 5: Project Data onto Principal Components\n",
        "\n",
        "**Purpose**: Transform the original data into the new lower-dimensional space.\n",
        "\n",
        "**Formula**:\n",
        "$$X_{\\text{PCA}} = X \\cdot V_k$$\n",
        "\n",
        "where:\n",
        "- $X_{\\text{PCA}}$ is the transformed data (n × k)\n",
        "- $X$ is the standardized original data (n × d)\n",
        "- $V_k$ is the matrix of top k eigenvectors (d × k)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0448efea",
      "metadata": {
        "id": "0448efea"
      },
      "source": [
        "## Matrix Decomposition\n",
        "\n",
        "Matrix decomposition techniques are fundamental to understanding PCA, as they provide the mathematical foundation for extracting principal components.\n",
        "\n",
        "### 1. Eigen-Decomposition:\n",
        "\n",
        "- Decomposes a square matrix **A** into:\n",
        "\n",
        "$$A = Q\\Lambda Q^{-1}$$\n",
        "\n",
        "where:\n",
        "- $Q$ = Matrix of eigenvectors.\n",
        "- $\\Lambda$ = Diagonal matrix of eigenvalues.\n",
        "\n",
        "**Use Case**: Solving systems of linear differential equations.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Singular Value Decomposition (SVD):\n",
        "\n",
        "- Decomposes any matrix **A** (m×n) into:\n",
        "\n",
        "$$A = U\\Sigma V^T$$\n",
        "\n",
        "where:\n",
        "- $U$ = Left singular vectors (eigenvectors of $AA^T$).\n",
        "- $\\Sigma$ = Diagonal matrix of singular values.\n",
        "- $V$ = Right singular vectors (eigenvectors of $A^T A$).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e303009e",
      "metadata": {
        "id": "e303009e"
      },
      "source": [
        "## Understanding Variance Ratio in Detail\n",
        "\n",
        "### What is Variance Ratio?\n",
        "\n",
        "A **variance ratio** is a statistical measure that compares two variances, most commonly in the context of a variance ratio F-test, which tests if the variances of two populations are equal or not by dividing the larger sample variance by the smaller one and comparing it to a critical value from the F-distribution. It can also refer to the proportion of total variance explained by each principal component in a PCA or a measure of the dispersion of events in a statistical distribution.\n",
        "\n",
        "### In Principal Component Analysis (PCA)\n",
        "\n",
        "**Purpose**:\n",
        "- To determine the optimal number of dimensions needed to explain the variance in a dataset.\n",
        "\n",
        "**Calculation**:\n",
        "- It is the ratio of the variance explained by a specific principal component to the total variance of the dataset.\n",
        "\n",
        "**Application**:\n",
        "- By analyzing the variance ratio for each principal component, one can select the components that contribute most to the overall variation in the data.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c503593",
      "metadata": {
        "id": "1c503593"
      },
      "source": [
        "## Advantages of PCA\n",
        "\n",
        "1. **Dimensionality Reduction**: Simplifies models without significant information loss\n",
        "2. **Noise Reduction**: Filters out irrelevant features\n",
        "3. **Visualization**: Enables 2D/3D plotting of high-dimensional data\n",
        "4. **Uncorrelated Features**: Principal components are orthogonal (uncorrelated)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c7ff46",
      "metadata": {
        "id": "14c7ff46"
      },
      "source": [
        "## Limitations of PCA\n",
        "\n",
        "1. **Linear Assumption**: PCA may fail with nonlinear relationships (Kernel PCA is an alternative)\n",
        "   - **Linear relationship**: Can be represented mathematically by a linear equation, such as $y = mx + b$\n",
        "   - **Nonlinear relationships**: Relationships between two variables that cannot be described by a straight line. Instead, they may follow a curve or some other pattern\n",
        "\n",
        "2. **Interpretability**: Principal components may not have clear real-world meaning\n",
        "\n",
        "3. **Sensitive to Scaling**: Features must be standardized\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb8c14a8",
      "metadata": {
        "id": "fb8c14a8"
      },
      "source": [
        "# Practical Examples\n",
        "\n",
        "## Example 1: Simple PCA Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16fda6ba",
      "metadata": {
        "id": "16fda6ba"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample data\n",
        "X = np.random.rand(100, 5)  # 100 samples, 5 features\n",
        "\n",
        "# Step 1: Standardize\n",
        "scaler = StandardScaler()\n",
        "X_std = scaler.fit_transform(X)\n",
        "\n",
        "# Step 2: Apply PCA\n",
        "pca = PCA(n_components=2)  # Reduce to 2D\n",
        "X_pca = pca.fit_transform(X_std)\n",
        "\n",
        "# Explained variance\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "print(\"Total Explained Variance:\", sum(pca.explained_variance_ratio_))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('PCA Projection')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d356281",
      "metadata": {
        "id": "8d356281"
      },
      "source": [
        "## Example 2: Eigenvalue Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "325a51b9",
      "metadata": {
        "id": "325a51b9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a square matrix\n",
        "A = np.array([[4, 2, 1],\n",
        "              [2, 5, 3],\n",
        "              [1, 3, 6]])\n",
        "\n",
        "print(\"Original matrix A:\")\n",
        "print(A)\n",
        "\n",
        "# Perform eigen decomposition\n",
        "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
        "\n",
        "print(\"\\nEigenvalues:\")\n",
        "print(eigenvalues)\n",
        "\n",
        "print(\"\\nEigenvectors (columns are eigenvectors):\")\n",
        "print(eigenvectors)\n",
        "\n",
        "# Create diagonal matrix of eigenvalues\n",
        "Lambda = np.diag(eigenvalues)\n",
        "\n",
        "print(\"\\nDiagonal matrix of eigenvalues (Λ):\")\n",
        "print(Lambda)\n",
        "\n",
        "# Reconstruct A using A = QΛQ⁻¹\n",
        "Q = eigenvectors\n",
        "Q_inv = np.linalg.inv(Q)\n",
        "A_reconstructed = Q @ Lambda @ Q_inv\n",
        "\n",
        "print(\"\\nReconstructed matrix A = QΛQ⁻¹:\")\n",
        "print(A_reconstructed)\n",
        "\n",
        "print(\"\\nVerification - Max absolute error:\", np.max(np.abs(A - A_reconstructed)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3216cae5",
      "metadata": {
        "id": "3216cae5"
      },
      "source": [
        "## Example 3: Singular Value Decomposition (SVD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "467fbb8b",
      "metadata": {
        "id": "467fbb8b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.random.rand(4, 3)  # Example matrix\n",
        "\n",
        "# Using numpy\n",
        "U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
        "\n",
        "print(\"U (left singular vectors):\\n\", U)\n",
        "print(\"\\nS (singular values):\\n\", S)\n",
        "print(\"\\nVt (right singular vectors transposed):\\n\", Vt)\n",
        "\n",
        "# Create diagonal matrix from singular values\n",
        "Sigma = np.diag(S)\n",
        "\n",
        "# Verify: A ≈ U @ Sigma @ Vt\n",
        "print(\"\\nVerification (A ≈ U @ Sigma @ Vt):\", np.allclose(A, U @ Sigma @ Vt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a1de9be",
      "metadata": {
        "id": "4a1de9be"
      },
      "source": [
        "## Applications of Matrix Decomposition\n",
        "\n",
        "Matrix decomposition techniques like Eigen-Decomposition and SVD have numerous real-world applications:\n",
        "\n",
        "### Key Applications:\n",
        "\n",
        "1. **Image Compression (JPEG)**: SVD is used to compress images by keeping only the most significant singular values, reducing storage requirements while maintaining visual quality.\n",
        "\n",
        "2. **Natural Language Processing (Latent Semantic Analysis)**: Matrix decomposition helps identify latent patterns and relationships in text data, enabling better document similarity and topic modeling.\n",
        "\n",
        "3. **Recommender Systems (Netflix, Amazon)**: SVD is fundamental to collaborative filtering algorithms that power recommendation engines, predicting user preferences based on historical data.\n",
        "\n",
        "4. **Medical Imaging (MRI)**: SVD helps in noise reduction and efficient storage of medical images, improving diagnostic capabilities while reducing storage costs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4797841",
      "metadata": {
        "id": "a4797841"
      },
      "source": [
        "---\n",
        "\n",
        "# Hands-On PCA Problems\n",
        "\n",
        "## Problem 1: Basic PCA with NumPy\n",
        "\n",
        "**Objective**: Implement PCA from scratch using NumPy on a synthetic dataset.\n",
        "\n",
        "**Steps**:\n",
        "1. Center the data\n",
        "2. Compute covariance matrix\n",
        "3. Perform eigendecomposition\n",
        "4. Sort eigenvalues and eigenvectors\n",
        "5. Project data to principal components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "436732ea",
      "metadata": {
        "id": "436732ea"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "mean = [0, 0]\n",
        "cov = [[1, 0.8], [0.8, 1]]\n",
        "X = np.random.multivariate_normal(mean, cov, 100)\n",
        "\n",
        "# 1. Center the data\n",
        "X_centered = X - np.mean(X, axis=0)\n",
        "\n",
        "# 2. Compute covariance matrix\n",
        "cov_matrix = np.cov(X_centered.T) # Same as np.cov(X_centered, rowvar=False)\n",
        "\n",
        "# 3. Perform eigendecomposition\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "# 4. Sort eigenvalues and eigenvectors\n",
        "sorted_idx = np.argsort(eigenvalues)[::-1]\n",
        "eigenvalues = eigenvalues[sorted_idx]\n",
        "eigenvectors = eigenvectors[:, sorted_idx]\n",
        "\n",
        "# 5. Project data to principal components\n",
        "principal_components = X_centered.dot(eigenvectors)\n",
        "\n",
        "print(\"Explained variance:\", eigenvalues)\n",
        "print(\"Principal components shape:\", principal_components.shape)\n",
        "print(\"\\nEigenvectors (Principal Component Directions):\")\n",
        "print(eigenvectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "386cd6e2",
      "metadata": {
        "id": "386cd6e2"
      },
      "source": [
        "## Problem 2: PCA with scikit-learn\n",
        "\n",
        "**Objective**: Use scikit-learn's PCA on the Iris dataset and visualize the results.\n",
        "\n",
        "**Steps**:\n",
        "1. Perform PCA with 2 components\n",
        "2. Plot the transformed data\n",
        "3. Print explained variance ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6e344c9",
      "metadata": {
        "id": "b6e344c9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 1. Perform PCA with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# 2. Plot the transformed data\n",
        "plt.figure(figsize=(10, 7))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=50, alpha=0.7, edgecolors='k')\n",
        "plt.xlabel('First Principal Component', fontsize=12)\n",
        "plt.ylabel('Second Principal Component', fontsize=12)\n",
        "plt.title('PCA of Iris Dataset', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Convert handles to a list\n",
        "handles, labels = scatter.legend_elements()\n",
        "plt.legend(handles=list(handles), labels=list(iris.target_names), title=\"Species\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Print explained variance ratio\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
        "print(\"Total explained variance:\", sum(pca.explained_variance_ratio_))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30686486",
      "metadata": {
        "id": "30686486"
      },
      "source": [
        "## Problem 3: PCA for Dimensionality Reduction\n",
        "\n",
        "**Objective**: Apply PCA for dimensionality reduction on the Digits dataset and evaluate classification performance.\n",
        "\n",
        "**Steps**:\n",
        "1. Perform PCA keeping 95% of variance\n",
        "2. Transform train and test data\n",
        "3. Train logistic regression on original and reduced data\n",
        "4. Compare accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbe469d9",
      "metadata": {
        "id": "fbe469d9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Perform PCA keeping 95% of variance\n",
        "pca = PCA(n_components=0.95)\n",
        "pca.fit(X_train)\n",
        "\n",
        "# 2. Transform train and test data\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "print(\"Original shape:\", X_train.shape)\n",
        "print(\"Reduced shape:\", X_train_pca.shape)\n",
        "print(f\"Number of components selected: {pca.n_components_}\")\n",
        "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.4f}\")\n",
        "\n",
        "# 3. Train logistic regression on original and reduced data\n",
        "# Original data\n",
        "lr_original = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_original.fit(X_train, y_train)\n",
        "y_pred_original = lr_original.predict(X_test)\n",
        "\n",
        "# Reduced data\n",
        "lr_pca = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = lr_pca.predict(X_test_pca)\n",
        "\n",
        "# 4. Compare accuracy\n",
        "acc_original = accuracy_score(y_test, y_pred_original)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(f\"\\nAccuracy with original data: {acc_original:.4f}\")\n",
        "print(f\"Accuracy with PCA-reduced data: {acc_pca:.4f}\")\n",
        "print(f\"Dimensionality reduction: {X_train.shape[1]} → {X_train_pca.shape[1]} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8fe53d",
      "metadata": {
        "id": "6d8fe53d"
      },
      "source": [
        "## Problem 4: PCA for Image Compression\n",
        "\n",
        "**Objective**: Use PCA to compress a grayscale image and analyze the reconstruction error.\n",
        "\n",
        "**Steps**:\n",
        "1. Perform PCA with increasing number of components\n",
        "2. Reconstruct image for each case\n",
        "3. Calculate and plot reconstruction error (MSE)\n",
        "4. Display reconstructed images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4213df07",
      "metadata": {
        "id": "4213df07"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_sample_image\n",
        "\n",
        "# Load sample image\n",
        "china = load_sample_image(\"china.jpg\")\n",
        "X = np.mean(china, axis=2) # Convert to grayscale\n",
        "X = X / 255.0  # Scale to [0, 1]\n",
        "\n",
        "# Flatten the image\n",
        "X_flat = X.reshape(-1, X.shape[1])\n",
        "\n",
        "# Try different numbers of components\n",
        "n_components = [1, 5, 10, 20, 50, 100, 200]\n",
        "mse_values = []\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.subplot(3, 3, 1)\n",
        "plt.imshow(X, cmap='gray')\n",
        "plt.title(\"Original\", fontsize=12, fontweight='bold')\n",
        "plt.axis('off')\n",
        "\n",
        "for i, n in enumerate(n_components):\n",
        "    # Perform PCA\n",
        "    pca = PCA(n_components=n)\n",
        "    X_pca = pca.fit_transform(X_flat)\n",
        "    X_reconstructed = pca.inverse_transform(X_pca)\n",
        "\n",
        "    # Calculate MSE\n",
        "    mse = np.mean((X_flat - X_reconstructed) ** 2)\n",
        "    mse_values.append(mse)\n",
        "\n",
        "    # Display reconstructed image\n",
        "    plt.subplot(3, 3, i+2)\n",
        "    plt.imshow(X_reconstructed.reshape(X.shape), cmap='gray')\n",
        "    plt.title(f\"n={n}\\nMSE={mse:.5f}\", fontsize=10)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot MSE vs n_components\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_components, mse_values, 'o-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Number of Components', fontsize=12)\n",
        "plt.ylabel('Reconstruction MSE', fontsize=12)\n",
        "plt.title('PCA Image Compression: MSE vs Number of Components', fontsize=14, fontweight='bold')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee456ff3",
      "metadata": {
        "id": "ee456ff3"
      },
      "source": [
        "---\n",
        "\n",
        "# Summary & Key Takeaways\n",
        "\n",
        "## Comprehensive Overview of Key Concepts\n",
        "\n",
        "| Concept | Definition | Application |\n",
        "|---------|-----------|-------------|\n",
        "| **Eigenvalues** | Scalars that scale eigenvectors | Stability analysis, Google PageRank |\n",
        "| **Eigenvectors** | Non-zero vectors scaled by eigenvalues | Facial recognition, Quantum mechanics |\n",
        "| **PCA** | Dimensionality reduction using eigenvalues | Finance, Image compression |\n",
        "| **Matrix Decomposition** | Breaking matrices into simpler forms (SVD, Eigen-decomposition) | MRI, Recommender systems |\n",
        "\n",
        "### Key Points to Remember:\n",
        "\n",
        "- **PCA** is a powerful tool for reducing dimensionality while preserving most of the variance in data\n",
        "- **Eigenvalues** and **eigenvectors** are fundamental to understanding how PCA works\n",
        "- **Variance ratio** helps determine how many principal components to retain\n",
        "- **Matrix decomposition** techniques (Eigen-decomposition and SVD) have wide-ranging applications across multiple domains\n",
        "- Always **standardize** your data before applying PCA to ensure all features contribute equally\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}